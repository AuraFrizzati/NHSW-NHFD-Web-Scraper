{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSY7zvQhCHLsAhP2s3p+OW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AuraFrizzati/NHSW-NHFD-Web-Scraper/blob/main/V2_GoogleColab_NHFD_WebScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NHFD web scraper (V2 - Dec 2025)**\n",
        "Script to web scrape Hip Fracture data from the [National Hip Fracture Database (NHFD)](https://www.nhfd.co.uk/)\n",
        "\n",
        "Press the play button in the cell below to run the webscraper. The data will be collated into 4 different tables (each table contains the data for *all* sites):\n",
        "- [Denoms](https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIsOverview) --> `NHFD_Data_Extraction_Collection_*.csv`\n",
        "- [Key Performance](https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs) --> `NHFD_KPI_*.csv`\n",
        "- [Length Of Stay](https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay) --> `NHFD_LOS_*.csv`\n",
        "- [Overall Performance](https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance) --> `NHFD_OP_*.csv`"
      ],
      "metadata": {
        "id": "rTd04tjgXL_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s1dvkI7aVxKZ",
        "outputId": "8cb6fff6-28fa-4d8b-8a0e-8ef1370075b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "------------------------------\n",
            "KPI webscraping started...\n",
            "------------------------------\n",
            "extracting KPI data for [ALL] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=[ALL] ...\n",
            "KPI data webscraping completed for [ALL]\n",
            "------------------------------\n",
            "extracting KPI data for [England] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=[England] ...\n",
            "KPI data webscraping completed for [England]\n",
            "------------------------------\n",
            "extracting KPI data for [Wales] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=[Wales] ...\n",
            "KPI data webscraping completed for [Wales]\n",
            "------------------------------\n",
            "extracting KPI data for [NI] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=[NI] ...\n",
            "KPI data webscraping completed for [NI]\n",
            "------------------------------\n",
            "extracting KPI data for BRG from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=BRG ...\n",
            "KPI data webscraping completed for BRG\n",
            "------------------------------\n",
            "extracting KPI data for CLW from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=CLW ...\n",
            "KPI data webscraping completed for CLW\n",
            "------------------------------\n",
            "extracting KPI data for GWE from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=GWE ...\n",
            "KPI data webscraping completed for GWE\n",
            "------------------------------\n",
            "extracting KPI data for GWY from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=GWY ...\n",
            "KPI data webscraping completed for GWY\n",
            "------------------------------\n",
            "extracting KPI data for MOR from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=MOR ...\n",
            "KPI data webscraping completed for MOR\n",
            "------------------------------\n",
            "extracting KPI data for PCH from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=PCH ...\n",
            "KPI data webscraping completed for PCH\n",
            "------------------------------\n",
            "extracting KPI data for POW from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=POW ...\n",
            "KPI data webscraping completed for POW\n",
            "------------------------------\n",
            "extracting KPI data for RGH from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=RGH ...\n",
            "KPI data webscraping completed for RGH\n",
            "------------------------------\n",
            "extracting KPI data for UHW from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=UHW ...\n",
            "KPI data webscraping completed for UHW\n",
            "------------------------------\n",
            "extracting KPI data for WRX from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=WRX ...\n",
            "KPI data webscraping completed for WRX\n",
            "------------------------------\n",
            "extracting KPI data for WWG from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=WWG ...\n",
            "KPI data webscraping completed for WWG\n",
            "------------------------------\n",
            "extracting KPI data for WYB from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=WYB ...\n",
            "KPI data webscraping completed for WYB\n",
            "KPI webscraping completed\n",
            "------------------------------\n",
            "------------------------------\n",
            "los webscraping started...\n",
            "------------------------------\n",
            "extracting LOS data for [ALL] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=[ALL] ...\n",
            "KPI data webscraping completed for [ALL]\n",
            "------------------------------\n",
            "extracting LOS data for [England] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=[England] ...\n",
            "KPI data webscraping completed for [England]\n",
            "------------------------------\n",
            "extracting LOS data for [Wales] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=[Wales] ...\n",
            "KPI data webscraping completed for [Wales]\n",
            "------------------------------\n",
            "extracting LOS data for [NI] from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=[NI] ...\n",
            "KPI data webscraping completed for [NI]\n",
            "------------------------------\n",
            "extracting LOS data for BRG from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=BRG ...\n",
            "KPI data webscraping completed for BRG\n",
            "------------------------------\n",
            "extracting LOS data for CLW from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=CLW ...\n",
            "KPI data webscraping completed for CLW\n",
            "------------------------------\n",
            "extracting LOS data for GWE from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=GWE ...\n",
            "KPI data webscraping completed for GWE\n",
            "------------------------------\n",
            "extracting LOS data for GWY from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=GWY ...\n",
            "KPI data webscraping completed for GWY\n",
            "------------------------------\n",
            "extracting LOS data for MOR from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=MOR ...\n",
            "KPI data webscraping completed for MOR\n",
            "------------------------------\n",
            "extracting LOS data for PCH from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=PCH ...\n",
            "KPI data webscraping completed for PCH\n",
            "------------------------------\n",
            "extracting LOS data for POW from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=POW ...\n",
            "KPI data webscraping completed for POW\n",
            "------------------------------\n",
            "extracting LOS data for RGH from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=RGH ...\n",
            "KPI data webscraping completed for RGH\n",
            "------------------------------\n",
            "extracting LOS data for UHW from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=UHW ...\n",
            "KPI data webscraping completed for UHW\n",
            "------------------------------\n",
            "extracting LOS data for WRX from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=WRX ...\n",
            "KPI data webscraping completed for WRX\n",
            "------------------------------\n",
            "extracting LOS data for WWG from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=WWG ...\n",
            "KPI data webscraping completed for WWG\n",
            "------------------------------\n",
            "extracting LOS data for WYB from https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=WYB ...\n",
            "KPI data webscraping completed for WYB\n",
            "LOS webscraping completed\n",
            "------------------------------\n",
            "------------------------------\n",
            "OP webscraping started...\n",
            "------------------------------\n",
            "extracting OP data for [ALL] from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=[ALL] ...\n",
            "KPI data webscraping completed for [ALL]\n",
            "------------------------------\n",
            "extracting OP data for [England] from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=[England] ...\n",
            "KPI data webscraping completed for [England]\n",
            "------------------------------\n",
            "extracting OP data for [Wales] from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=[Wales] ...\n",
            "KPI data webscraping completed for [Wales]\n",
            "------------------------------\n",
            "extracting OP data for [NI] from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=[NI] ...\n",
            "KPI data webscraping completed for [NI]\n",
            "------------------------------\n",
            "extracting OP data for BRG from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=BRG ...\n",
            "KPI data webscraping completed for BRG\n",
            "------------------------------\n",
            "extracting OP data for CLW from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=CLW ...\n",
            "KPI data webscraping completed for CLW\n",
            "------------------------------\n",
            "extracting OP data for GWE from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=GWE ...\n",
            "KPI data webscraping completed for GWE\n",
            "------------------------------\n",
            "extracting OP data for GWY from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=GWY ...\n",
            "KPI data webscraping completed for GWY\n",
            "------------------------------\n",
            "extracting OP data for MOR from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=MOR ...\n",
            "KPI data webscraping completed for MOR\n",
            "------------------------------\n",
            "extracting OP data for PCH from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=PCH ...\n",
            "KPI data webscraping completed for PCH\n",
            "------------------------------\n",
            "extracting OP data for POW from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=POW ...\n",
            "KPI data webscraping completed for POW\n",
            "------------------------------\n",
            "extracting OP data for RGH from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=RGH ...\n",
            "KPI data webscraping completed for RGH\n",
            "------------------------------\n",
            "extracting OP data for UHW from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=UHW ...\n",
            "KPI data webscraping completed for UHW\n",
            "------------------------------\n",
            "extracting OP data for WRX from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=WRX ...\n",
            "KPI data webscraping completed for WRX\n",
            "------------------------------\n",
            "extracting OP data for WWG from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=WWG ...\n",
            "KPI data webscraping completed for WWG\n",
            "------------------------------\n",
            "extracting OP data for WYB from https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=WYB ...\n",
            "KPI data webscraping completed for WYB\n",
            "OP webscraping completed\n",
            "------------------------------\n",
            "------------------------------\n",
            "Denoms webscraping started...\n",
            "Denoms webscraping completed\n",
            "------------------------------\n",
            "------------------------------\n",
            "All files saved via Colab to Files directory: NHFD_data\n",
            "NHFD_LOS_2025-12-03__09_21_54.csv\n",
            "NHFD_OP_2025-12-03__09_21_54.csv\n",
            "NHFD_Data_Extraction_Collection_2025-12-03__09_21_54.csv\n",
            "NHFD_KPI_2025-12-03__09_21_54.csv\n",
            "------------------------------\n",
            "------------------------------\n",
            "Wait for NHFD_data to be zipped...\n",
            "Successfully created 'NHFD_data.zip' from 'NHFD_data'\n",
            "Process completed.\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import os\n",
        "import re\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util import Retry\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "## manual selection of websites to scrape (if True, the specific dataset will be downloaded, change to False if you do not want to download it)\n",
        "denoms = True\n",
        "kpi = True\n",
        "los = True\n",
        "op = True\n",
        "\n",
        "def setup():\n",
        "    # Create a fixed directory path to save the file\n",
        "    directory_path = 'NHFD_data'\n",
        "    # Ensure the directory exists (create it if needed)\n",
        "    os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "    # setting up timestamp\n",
        "    date_of_scrape = datetime.now(pytz.timezone(\"Europe/London\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    date_of_scrape_file_name_tag =date_of_scrape.replace(\":\", \"_\").replace(\" \", \"__\")\n",
        "\n",
        "    # Hospital sites of interest\n",
        "    site_list = [\n",
        "    \"[ALL]\",\n",
        "    \"[England]\",\n",
        "    \"[Wales]\",\n",
        "    \"[NI]\",\n",
        "    \"BRG\",\n",
        "    \"CLW\",\n",
        "    \"GWE\",\n",
        "    \"GWY\",\n",
        "    \"MOR\",\n",
        "    \"PCH\",\n",
        "    \"POW\",\n",
        "    \"RGH\",\n",
        "    \"UHW\",\n",
        "    \"WRX\",\n",
        "    \"WWG\",\n",
        "    \"WYB\"\n",
        "    ]\n",
        "\n",
        "    # Define the retry strategy\n",
        "    # https://www.zenrows.com/blog/python-requests-retry#status-codes\n",
        "    retry_strategy = Retry(\n",
        "        total=4,  # Maximum number of retries\n",
        "        status_forcelist=[429, ## Too Many Requests\n",
        "                        500, ## Internal Server Error\n",
        "                        502, ## 502 Bad Gateway\n",
        "                        503, ## 503 Service Unavailable\n",
        "                        504 ## 504 Gateway Timeout\n",
        "                        ],  # HTTP status codes to retry on\n",
        "    )\n",
        "    # Create an HTTP adapter with the retry strategy and mount it to session\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "\n",
        "    # Create a new session object and attach the retry strategy to it\n",
        "    session = requests.Session()\n",
        "    session.mount('https://', adapter)\n",
        "    session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
        "    return directory_path, date_of_scrape,date_of_scrape_file_name_tag, site_list, session\n",
        "\n",
        "def scrape_plot_data(\n",
        "        category,\n",
        "        # directory_path,\n",
        "        date_of_scrape,\n",
        "        site_list,\n",
        "        session,\n",
        "        div,\n",
        "        url,\n",
        "        html_labels,\n",
        "        columns_order,\n",
        "        columns_names):\n",
        "\n",
        "    combined_frames = []\n",
        "\n",
        "    for x in range(len(site_list)):\n",
        "        print(div)\n",
        "        complete_url = f\"{url}{site_list[x]}\"\n",
        "        print(f\"extracting {category} data for {site_list[x]} from {complete_url} ...\")\n",
        "        try:\n",
        "            result = session.get(url)\n",
        "            result.raise_for_status()  # Raises HTTPError for bad responses (4xx, 5xx)\n",
        "\n",
        "            doc = BeautifulSoup(result.text, \"html.parser\")  # Parse Content\n",
        "            scripts = '\\n'.join([script.text for script in doc.find_all(\"script\")])\n",
        "\n",
        "            # Regex to extract data as tuples with 3 elements: ((before =),(=),(after = and up to ;)).\n",
        "            # view-source example:https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=WWG (DATA1 first block)\n",
        "            # Example: ( (var cats), (=), (['Jan 2017','Feb 2017','Mar 2017','Apr 2017'...];) )\n",
        "            # The 3-part tuple for each list item will always match the below Index positions:\n",
        "            # [0] = variable name (i.e. var cat)\n",
        "            # [1] = will always be the string '='\n",
        "            # [2] = the content of the variable (a series of values)\n",
        "            match_scripts = re.findall(r\"(.*)( = )([^;]*)\", scripts)\n",
        "\n",
        "            # labels to filter match scripts via their first element [0] (these contain the data)\n",
        "            match_scripts_labels = html_labels\n",
        "\n",
        "            # Extract data as a dictionary using list comprehension, in the form {match_scripts_label: [data series]}\n",
        "            df_dict = {\n",
        "                match_scripts[i][0].strip(): [val.strip() for val in match_scripts[i][2].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\"null\", \"\").strip().split(\",\")]\n",
        "                for i in range(len(match_scripts))\n",
        "                if match_scripts[i][0] in match_scripts_labels\n",
        "            }\n",
        "\n",
        "            # check length of values expected (some series are empty/shorter at the end and require padding wiht blank values)\n",
        "            max_length = max(len(values) for values in df_dict.values())\n",
        "            # Pad all lists in the dictionary to match the maximum length\n",
        "            df_dict = {\n",
        "                key: values + [\"\"] * (max_length - len(values))\n",
        "                for key, values in df_dict.items()\n",
        "            }\n",
        "\n",
        "            # convert the dictionary to pandas df\n",
        "            df = pd.DataFrame(df_dict)\n",
        "\n",
        "            new_order = columns_order\n",
        "            df = df[new_order]\n",
        "\n",
        "            # Rename Columns to match the NHFD website\n",
        "            df.rename(columns = columns_names,\n",
        "            inplace=True\n",
        "            )\n",
        "            df[\"file_name\"] = f'NHFD_{site_list[x]}_{category}_{date_of_scrape.replace(\":\", \"_\")}.csv'\n",
        "            df[\"upload_date\"] = date_of_scrape\n",
        "            print(f\"KPI data webscraping completed for {site_list[x]}\")\n",
        "            combined_frames.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data for {site_list[x]}: {e}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    return combined_frames\n",
        "\n",
        "\n",
        "def denoms_webscraper(directory_path, date_of_scrape, site_list, session, div):\n",
        "    combined_frames = []\n",
        "\n",
        "    ### [1a] Webscrape Denoms\n",
        "    sentences_list = []\n",
        "    month_end_list = []\n",
        "    cases_list = []\n",
        "    k0_list = []\n",
        "    k1_list = []\n",
        "    k2_list = []\n",
        "    k3_list = []\n",
        "    k4_list = []\n",
        "    k5_list = []\n",
        "    k6_list = []\n",
        "    k7_list = []\n",
        "\n",
        "    try:\n",
        "        #print(div)\n",
        "        #print(f\"extracting Denoms data...\")\n",
        "        for x in range(len(site_list)):\n",
        "\n",
        "            url = f\"https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIsOverview?open&org={site_list[x]}\"\n",
        "\n",
        "            result = session.get(url)\n",
        "            result.raise_for_status()  # Raises HTTPError for bad responses (4xx, 5xx)\n",
        "\n",
        "            doc = BeautifulSoup(result.text, \"html.parser\")  # Parse Content\n",
        "\n",
        "            # [1] find the descriptive text in the html code\n",
        "            # e.g. \"Annualised values based on 70,617 cases averaged over 12 months..\"\n",
        "            sentence = doc.find_all('div', {'class': 'note'})[0].text.replace('\\n', '')\n",
        "            sentences_list.append(sentence)\n",
        "\n",
        "            # [2] find the KPI numeric data in the html code\n",
        "            data = doc.find_all('div', {'class': 'annual'})\n",
        "            kpi_data = []\n",
        "\n",
        "            for i in data:\n",
        "                kpi_data.append(i.text)\n",
        "\n",
        "            # [3] Create lists by KPI type\n",
        "            k0_list.append(kpi_data[0])\n",
        "            k1_list.append(kpi_data[1])\n",
        "            k2_list.append(kpi_data[2])\n",
        "            k3_list.append(kpi_data[3])\n",
        "            k4_list.append(kpi_data[4])\n",
        "            k5_list.append(kpi_data[5])\n",
        "            k6_list.append(kpi_data[6])\n",
        "            k7_list.append(kpi_data[7])\n",
        "\n",
        "            # [4] Extract Month and Year from descriptive text webscraped in [1]\n",
        "\n",
        "            # Look for one or more word characters `(\\w+)`, followed by one or more whitespaces `\\s+`,\n",
        "            # followed by exactly 4 digits `(\\d{4})`\n",
        "            match = re.search(r'(\\w+)\\s+(\\d{4})', sentence)\n",
        "\n",
        "            # Get the word and the 4 digits groups and divide them by a blank space\n",
        "            month_year_str = match.group(1) + ' ' + match.group(2)\n",
        "\n",
        "            #  Use `datetime.strptime` to parse the \"Month YYYY\" as a date,\n",
        "            # then use `.strftime` to convert it to a string with the format yyyy-mm-dd\n",
        "            month_year_date = datetime.strptime(month_year_str, \"%B %Y\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            month_end_list.append(month_year_date)\n",
        "\n",
        "            # [5] Extract the number of cases from descriptive text webscraped in [1]\n",
        "            cases_list.append(sentence.partition(\"on\")[2].split()[0])\n",
        "\n",
        "        NHFD_KPI_collection_list = []\n",
        "\n",
        "        for i in range(len(site_list)):\n",
        "\n",
        "            NHFD_KPI_collection_list.append(\n",
        "                [\n",
        "                    site_list[i],\n",
        "                    # sentences_list[i], ## this was originally downloaded but possibly not required anymore?\n",
        "                    month_end_list[i],\n",
        "                    cases_list[i],\n",
        "                    k0_list[i],\n",
        "                    k1_list[i],\n",
        "                    k2_list[i],\n",
        "                    k3_list[i],\n",
        "                    k4_list[i],\n",
        "                    k5_list[i],\n",
        "                    k6_list[i],\n",
        "                    k7_list[i],\n",
        "                    date_of_scrape\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        col_names = [\"url\",\"month_end\",\"cases\",\"k0\",\"k1\",\"k2\",\"k3\",\"k4\",\"k5\",\"k6\",\"k7\",\"date_of_scrape\"]\n",
        "        df = pd.DataFrame(NHFD_KPI_collection_list, columns=col_names)\n",
        "\n",
        "        ## Remove the grouping quotes from cases numbers and convert them into integers\n",
        "        df[\"cases\"] = df[\"cases\"].str.replace(\",\", \"\").astype(int)\n",
        "\n",
        "        ## Convert KPI data from percentages into decimal numbers (proportions)\n",
        "        for column in df.columns:\n",
        "            if column.startswith(\"k\"):\n",
        "                df[column] = df[column].astype(\"string\").str.replace(\"%\", \"\").astype(float)/100\n",
        "\n",
        "\n",
        "        df[\"upload_date\"] = date_of_scrape\n",
        "        #print(f\"Denoms data webscraping completed\")\n",
        "        combined_frames.append(df)\n",
        "\n",
        "    except Exception as e:\n",
        "            print(f\"Error encountered: {e}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    return combined_frames\n",
        "\n",
        "def main():\n",
        "\n",
        "  div = \"------------------------------\"\n",
        "\n",
        "  directory_path, date_of_scrape,date_of_scrape_file_name_tag, site_list, session = setup()\n",
        "\n",
        "      # Configuration dictionary for each data type\n",
        "  scraper_config = {\n",
        "      'KPI': {\n",
        "          'url': \"https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/KPIs?open&org=\",\n",
        "          'match_scripts_labels': [\"var cats\", \"var kpi0\", \"var kpi0a\", \"var kpi1\", \"var kpi1a\",\n",
        "                                    \"var kpi2\", \"var kpi2a\", \"var kpi3\", \"var kpi3a\", \"var kpi4\",\n",
        "                                    \"var kpi4a\", \"var kpi5\", \"var kpi5a\", \"var kpi6\", \"var kpi6a\",\n",
        "                                    \"var kpi7\", \"var kpi7a\"],\n",
        "          'columns_order': [\"var cats\", \"var kpi0\", \"var kpi0a\", \"var kpi1\", \"var kpi1a\", \"var kpi2\", \"var kpi2a\",\n",
        "                            \"var kpi3\", \"var kpi3a\", \"var kpi4\", \"var kpi4a\", \"var kpi5\", \"var kpi5a\", \"var kpi6\",\n",
        "                            \"var kpi6a\", \"var kpi7\", \"var kpi7a\"],\n",
        "          'columns_names': {\n",
        "              \"var cats\": \"admission_year_and_month\",\n",
        "              \"var kpi0\": \"admission_to_specialist_ward_pct_k0\",\n",
        "              \"var kpi0a\": \"admission_to_specialist_ward_pct_k0_annual\",\n",
        "              \"var kpi1\": \"prompt_orthogeriatric_review_pct_k1\",\n",
        "              \"var kpi1a\": \"prompt_orthogeriatric_review_pct_k1_annual\",\n",
        "              \"var kpi2\": \"prompt_surgery_pct_k2\",\n",
        "              \"var kpi2a\": \"prompt_surgery_pct_k2_annual\",\n",
        "              \"var kpi3\": \"nice_compliant_surgery_pct_k3\",\n",
        "              \"var kpi3a\": \"nice_compliant_surgery_pct_k3_annual\",\n",
        "              \"var kpi4\": \"prompt_mobilisation_pct_k4\",\n",
        "              \"var kpi4a\": \"prompt_mobilisation_pct_k4_annual\",\n",
        "              \"var kpi5\": \"not_delirious_postop_pct_k5\",\n",
        "              \"var kpi5a\": \"not_delirious_postop_pct_k5_annual\",\n",
        "              \"var kpi6\": \"return_to_original_residence_pct_k6\",\n",
        "              \"var kpi6a\": \"return_to_original_residence_pct_k6_annual\",\n",
        "              \"var kpi7\": \"bone_protection_medication_pct_k7\",\n",
        "              \"var kpi7a\": \"bone_protection_medication_percentage_k7_annual\"\n",
        "          }\n",
        "      },\n",
        "      'LOS': {\n",
        "          'url': \"https://www.nhfd.co.uk/20/NHFDcharts.nsf/vwCharts/LengthOfStay?open&org=\",\n",
        "          'match_scripts_labels': [\"var cats\", \"var cases\", \"var Ward\", \"var WardAve\", \"var Trust\",\n",
        "                                    \"var TrustAve\", \"var TTW\", \"var TTWAve\"],\n",
        "          'columns_order': [\"var cats\", \"var cases\", \"var Ward\", \"var WardAve\", \"var Trust\",\n",
        "                            \"var TrustAve\", \"var TTW\", \"var TTWAve\"],\n",
        "          'columns_names': {\n",
        "              \"var cats\": \"trust_discharge_year_and_month\",\n",
        "              \"var cases\": \"patients_number_per_month\",\n",
        "              \"var Ward\": \"acute_hospital_length_of_stay_days\",\n",
        "              \"var WardAve\": \"acute_hospital_length_of_stay_days_annual\",\n",
        "              \"var Trust\": \"overall_trust_length_of_stay_days\",\n",
        "              \"var TrustAve\": \"overall_trust_length_of_stay_days_annual\",\n",
        "              \"var TTW\": \"emergency_department_outlier_length_of_stay_days\",\n",
        "              \"var TTWAve\": \"emergency_department_outlier_length_of_stay_days_annual\"\n",
        "          }\n",
        "      },\n",
        "      'OP': {\n",
        "          'url': \"https://www.nhfd.co.uk/20/NHFDCharts.nsf/Charts/OverallPerformance?open&org=\",\n",
        "          'match_scripts_labels': [\"var cats\", \"var cases\", \"var TTS\", \"var TTSAve\", \"var TTSAve2\",\n",
        "                                    \"var MortPC\", \"var MortAve\", \"var MortAve2\"],\n",
        "          'columns_order': [\"var cats\", \"var cases\", \"var TTS\", \"var TTSAve\", \"var TTSAve2\",\n",
        "                            \"var MortPC\", \"var MortAve\", \"var MortAve2\"],\n",
        "          'columns_names': {\n",
        "              \"var cats\": \"admission_year_and_month\",\n",
        "              \"var cases\": \"patients\",\n",
        "              \"var TTS\": \"hours_to_operation\",\n",
        "              \"var TTSAve\": \"hours_to_operation_annual\",\n",
        "              \"var TTSAve2\": \"hours_to_operation_national\",\n",
        "              \"var MortPC\": \"30_day_mortality_pct\",\n",
        "              \"var MortAve\": \"30_day_mortality_pct_annual\",\n",
        "              \"var MortAve2\": \"30_day_mortality_pct_national\"\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "\n",
        "  if (kpi):\n",
        "      # initialise variables\n",
        "      category = 'KPI'\n",
        "      config = scraper_config[category]\n",
        "\n",
        "      print(div)\n",
        "      print(div)\n",
        "      print(f'{category} webscraping started...')\n",
        "      combined_file_name = f\"NHFD_{category}_{date_of_scrape_file_name_tag}.csv\"\n",
        "      combined_file_path = os.path.join(directory_path, combined_file_name)\n",
        "\n",
        "      combined_frames_KPI = scrape_plot_data(\n",
        "          category,\n",
        "          date_of_scrape,\n",
        "          site_list,\n",
        "          session,\n",
        "          div,\n",
        "          config['url'],\n",
        "          config['match_scripts_labels'],\n",
        "          config['columns_order'],\n",
        "          config['columns_names']\n",
        "          )\n",
        "\n",
        "      final_df = pd.concat(combined_frames_KPI, ignore_index=True)\n",
        "      final_df.to_csv(\n",
        "          combined_file_path,\n",
        "          index=False\n",
        "      )\n",
        "\n",
        "      print(f'{category} webscraping completed')\n",
        "\n",
        "\n",
        "  if (los):\n",
        "      # initialise variables\n",
        "      category = 'LOS'\n",
        "      config = scraper_config[category]\n",
        "\n",
        "      print(div)\n",
        "      print(div)\n",
        "      print('los webscraping started...')\n",
        "      combined_file_name = f\"NHFD_LOS_{date_of_scrape_file_name_tag}.csv\"\n",
        "      combined_file_path = os.path.join(directory_path, combined_file_name)\n",
        "\n",
        "      combined_frames_LOS = scrape_plot_data(\n",
        "          category,\n",
        "          date_of_scrape,\n",
        "          site_list,\n",
        "          session,\n",
        "          div,\n",
        "          config['url'],\n",
        "          config['match_scripts_labels'],\n",
        "          config['columns_order'],\n",
        "          config['columns_names']\n",
        "          )\n",
        "\n",
        "\n",
        "      final_df = pd.concat(combined_frames_LOS, ignore_index=True)\n",
        "      final_df.to_csv(\n",
        "          combined_file_path,\n",
        "          index=False\n",
        "          )\n",
        "\n",
        "      print('LOS webscraping completed')\n",
        "\n",
        "  if (op):\n",
        "      # initialise variables\n",
        "      category = 'OP'\n",
        "      config = scraper_config[category]\n",
        "\n",
        "      print(div)\n",
        "      print(div)\n",
        "      print('OP webscraping started...')\n",
        "      combined_file_name = f\"NHFD_OP_{date_of_scrape_file_name_tag}.csv\"\n",
        "      combined_file_path = os.path.join(directory_path, combined_file_name)\n",
        "\n",
        "      combined_frames_OP = scrape_plot_data(\n",
        "          category,\n",
        "          date_of_scrape,\n",
        "          site_list,\n",
        "          session,\n",
        "          div,\n",
        "          config['url'],\n",
        "          config['match_scripts_labels'],\n",
        "          config['columns_order'],\n",
        "          config['columns_names']\n",
        "      )\n",
        "\n",
        "      final_df = pd.concat(combined_frames_OP, ignore_index=True)\n",
        "      final_df.to_csv(\n",
        "          combined_file_path,\n",
        "          index=False\n",
        "      )\n",
        "\n",
        "      print('OP webscraping completed')\n",
        "\n",
        "  if (denoms):\n",
        "      print(div)\n",
        "      print(div)\n",
        "      print('Denoms webscraping started...')\n",
        "      combined_file_name = f\"NHFD_Data_Extraction_Collection_{date_of_scrape_file_name_tag}.csv\"\n",
        "      combined_file_path = os.path.join(directory_path, combined_file_name)\n",
        "\n",
        "      combined_frames_denoms = denoms_webscraper(directory_path, date_of_scrape, site_list, session, div)\n",
        "\n",
        "\n",
        "      final_df = pd.concat(combined_frames_denoms, ignore_index=True)\n",
        "      final_df.to_csv(\n",
        "          combined_file_path,\n",
        "          index=False\n",
        "          #,header=False\n",
        "      )\n",
        "\n",
        "      print('Denoms webscraping completed')\n",
        "\n",
        "  print(div)\n",
        "  print(div)\n",
        "  print(f\"All files saved via Colab to Files directory: {directory_path}\")\n",
        "  for file_name in os.listdir(directory_path):\n",
        "    print(file_name)\n",
        "  print(div)\n",
        "  print(div)\n",
        "  print(f\"Wait for {directory_path} to be zipped...\")\n",
        "\n",
        "  # Define the folder name to be zipped\n",
        "  folder_to_zip = directory_path\n",
        "\n",
        "  # Define the output zip file name\n",
        "  output_zip_name = directory_path\n",
        "\n",
        "  # Check if the folder exists in the current directory\n",
        "  if os.path.exists(directory_path):\n",
        "      try:\n",
        "          # Create a zip archive of the folder\n",
        "          shutil.make_archive(directory_path, 'zip', directory_path)\n",
        "          print(f\"Successfully created '{directory_path}.zip' from '{directory_path}'\")\n",
        "          print(\"Process completed.\")\n",
        "          print(div)\n",
        "      except Exception as e:\n",
        "          print(f\"Error zipping folder '{directory_path}': {e}\")\n",
        "  else:\n",
        "      print(f\"Folder '{directory_path}' not found in the current directory. No zip archive was created.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}